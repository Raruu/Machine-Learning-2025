{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2d9a28",
   "metadata": {},
   "source": [
    "# Praktikum 6\n",
    "\n",
    "Lakukan percobaan penggunaan ANNOY, FAISS, dan HNSWLIB pada dataset sekunder berukuran besar (Micro Spotify) pada link berikut: https://www.kaggle.com/datasets/bwandowando/spotify-songs-with-attributes-and-lyrics/data . Download data dan load CSV filenya (pilih dataset yg pertama dari dua dataset). pilih hanya fitur numerik saja, dan lakukan normalisasi menggunakan StandardScaler. Lakukan pencarian track terdekat dan bandingkan hasilnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc0f8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\NaniKaSuru\\Polinema\\Machine-Learning-2025\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/bwandowando/spotify-songs-with-attributes-and-lyrics?dataset_version_number=19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 894M/894M [01:29<00:00, 10.5MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Raruu\\.cache\\kagglehub\\datasets\\bwandowando\\spotify-songs-with-attributes-and-lyrics\\versions\\19\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"bwandowando/spotify-songs-with-attributes-and-lyrics\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "!pip install -q annoy faiss-cpu hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa3030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact NN (queries=1000) done in 345.455 s\n",
      "Annoy build: 11.288 s, query all: 40.360 s\n",
      "Annoy build: 11.288 s, query all: 40.360 s\n",
      "HNSW build: 17.618 s, query all: 16.006 s\n",
      "HNSW build: 17.618 s, query all: 16.006 s\n",
      "FAISS build: 0.124 s, query all: 73.317 s\n",
      "FAISS build: 0.124 s, query all: 73.317 s\n",
      "\n",
      "Summary (build time | query time for sampled points | recall@k)\n",
      "Exact:  - | 345.455 s (queries only) | recall=1.00\n",
      "Annoy:  11.288 s | 40.360 s | recall@10=0.9945\n",
      "HNSW:   17.618 s | 16.006 s | recall@10=0.9936\n",
      "FAISS:  0.124 s | 73.317 s | recall@10=0.9982\n",
      "\n",
      "Top-5 neighbors for first sampled query (dataset index = 287796)\n",
      "Exact NN: [     0 394553 764272 837727 749223]\n",
      "Annoy:    [0, 394553, 764272, 837727, 749223]\n",
      "HNSW:     [     0 394553 764272 837727 749223]\n",
      "FAISS:    [     0 394553 764272 837727 749223]\n",
      "\n",
      "Summary (build time | query time for sampled points | recall@k)\n",
      "Exact:  - | 345.455 s (queries only) | recall=1.00\n",
      "Annoy:  11.288 s | 40.360 s | recall@10=0.9945\n",
      "HNSW:   17.618 s | 16.006 s | recall@10=0.9936\n",
      "FAISS:  0.124 s | 73.317 s | recall@10=0.9982\n",
      "\n",
      "Top-5 neighbors for first sampled query (dataset index = 287796)\n",
      "Exact NN: [     0 394553 764272 837727 749223]\n",
      "Annoy:    [0, 394553, 764272, 837727, 749223]\n",
      "HNSW:     [     0 394553 764272 837727 749223]\n",
      "FAISS:    [     0 394553 764272 837727 749223]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import faiss\n",
    "from annoy import AnnoyIndex\n",
    "import hnswlib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use all available CPU cores where possible\n",
    "n_cores = os.cpu_count() or 1\n",
    "os.environ.setdefault('OMP_NUM_THREADS', str(n_cores))\n",
    "os.environ.setdefault('OPENBLAS_NUM_THREADS', str(n_cores))\n",
    "os.environ.setdefault('MKL_NUM_THREADS', str(n_cores))\n",
    "# Tell faiss to use multiple threads (if built with OpenMP)\n",
    "try:\n",
    "    faiss.omp_set_num_threads(n_cores)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------------\n",
    "# Load dataset (drop NaNs in chosen features)\n",
    "# -------------------------------\n",
    "df = pd.read_csv(f'{path}/songs_with_attributes_and_lyrics.csv')  # ganti path sesuai lokasi file\n",
    "features = ['danceability', 'energy', 'loudness', 'speechiness',\n",
    "            'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo']\n",
    "df = df[features].dropna().reset_index(drop=True)\n",
    "X = df.values\n",
    "\n",
    "# Standardize and cast to float32 (required by faiss/hnswlib)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X).astype(np.float32)\n",
    "\n",
    "n = X_scaled.shape[0]\n",
    "k = 10  # jumlah nearest neighbors\n",
    "# To keep this runnable on limited RAM, sample up to 1000 query points\n",
    "n_queries = min(1000, n)\n",
    "rng = np.random.default_rng(42)\n",
    "query_idx = rng.choice(n, size=n_queries, replace=False)\n",
    "# Xq = X_scaled[query_idx]\n",
    "Xq = X_scaled\n",
    "\n",
    "# -------------------------------\n",
    "# Exact Nearest Neighbor (brute-force) - only for the sampled queries\n",
    "# -------------------------------\n",
    "t0 = time.time()\n",
    "nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean', n_jobs=-1)\n",
    "nn.fit(X_scaled)\n",
    "dist_exact, idx_exact = nn.kneighbors(Xq)\n",
    "time_exact = time.time() - t0\n",
    "print(f\"Exact NN (queries={n_queries}) done in {time_exact:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# Annoy (build + query on sampled points)\n",
    "# -------------------------------\n",
    "t0 = time.time()\n",
    "fdim = X_scaled.shape[1]\n",
    "index_annoy = AnnoyIndex(fdim, 'euclidean')\n",
    "for i, v in enumerate(X_scaled):\n",
    "    index_annoy.add_item(i, v.tolist())\n",
    "n_trees = 50\n",
    "index_annoy.build(n_trees)\n",
    "t_build_annoy = time.time() - t0\n",
    "\n",
    "tq = time.time()\n",
    "# Annoy: parallelize queries using joblib (threading) to utilize multiple cores\n",
    "def _query_annoy(v):\n",
    "    return index_annoy.get_nns_by_vector(v.tolist(), k)\n",
    "idx_annoy = Parallel(n_jobs=n_cores, prefer='threads')(delayed(_query_annoy)(v) for v in Xq)\n",
    "time_query_annoy = time.time() - tq\n",
    "print(f\"Annoy build: {t_build_annoy:.3f} s, query all: {time_query_annoy:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# HNSW (hnswlib)\n",
    "# -------------------------------\n",
    "t0 = time.time()\n",
    "p = hnswlib.Index(space='l2', dim=fdim)\n",
    "p.init_index(max_elements=n, ef_construction=200, M=16)\n",
    "p.add_items(X_scaled)\n",
    "p.set_ef(200)\n",
    "t_build_hnsw = time.time() - t0\n",
    "\n",
    "tq = time.time()\n",
    "# hnswlib supports num_threads in knn_query\n",
    "idx_hnsw, dist_hnsw = p.knn_query(Xq, k=k, num_threads=n_cores)\n",
    "time_query_hnsw = time.time() - tq\n",
    "print(f\"HNSW build: {t_build_hnsw:.3f} s, query all: {time_query_hnsw:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# FAISS IVF (train on full set, query sampled points)\n",
    "# -------------------------------\n",
    "t0 = time.time()\n",
    "quantizer = faiss.IndexFlatL2(fdim)\n",
    "nlist = 100\n",
    "index_faiss = faiss.IndexIVFFlat(quantizer, fdim, nlist, faiss.METRIC_L2)\n",
    "# FAISS requires float32 and contiguous arrays\n",
    "index_faiss.train(np.ascontiguousarray(X_scaled))\n",
    "index_faiss.add(np.ascontiguousarray(X_scaled))\n",
    "index_faiss.nprobe = 10\n",
    "t_build_faiss = time.time() - t0\n",
    "\n",
    "tq = time.time()\n",
    "# FAISS can use multiple threads via set_num_threads if available\n",
    "try:\n",
    "    faiss.omp_set_num_threads(n_cores)\n",
    "except Exception:\n",
    "    pass\n",
    "D_faiss, idx_faiss = index_faiss.search(np.ascontiguousarray(Xq), k)\n",
    "time_query_faiss = time.time() - tq\n",
    "print(f\"FAISS build: {t_build_faiss:.3f} s, query all: {time_query_faiss:.3f} s\")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate recall@k for each ANN vs exact\n",
    "# -------------------------------\n",
    "def recall_at_k(true_idx, pred_idx, k):\n",
    "    # true_idx: (n_queries, k), pred_idx: iterable of length n_queries with lists/arrays\n",
    "    total = 0.0\n",
    "    n = len(true_idx)\n",
    "    for t, p in zip(true_idx, pred_idx):\n",
    "        pset = set(p.tolist() if hasattr(p, 'tolist') else p)\n",
    "        total += len(pset.intersection(set(t[:k]))) / float(k)\n",
    "    return total / n\n",
    "\n",
    "rec_annoy = recall_at_k(idx_exact, idx_annoy, k)\n",
    "rec_hnsw = recall_at_k(idx_exact, idx_hnsw, k)\n",
    "rec_faiss = recall_at_k(idx_exact, idx_faiss, k)\n",
    "\n",
    "print('\\nSummary (build time | query time for sampled points | recall@k)')\n",
    "print(f\"Exact:  - | {time_exact:.3f} s (queries only) | recall=1.00\")\n",
    "print(f\"Annoy:  {t_build_annoy:.3f} s | {time_query_annoy:.3f} s | recall@{k}={rec_annoy:.4f}\")\n",
    "print(f\"HNSW:   {t_build_hnsw:.3f} s | {time_query_hnsw:.3f} s | recall@{k}={rec_hnsw:.4f}\")\n",
    "print(f\"FAISS:  {t_build_faiss:.3f} s | {time_query_faiss:.3f} s | recall@{k}={rec_faiss:.4f}\")\n",
    "\n",
    "# show top-5 neighbors for the first sampled query (original dataset index)\n",
    "qid = query_idx[0]\n",
    "print(\"\\nTop-5 neighbors for first sampled query (dataset index = {})\".format(int(qid)))\n",
    "print(f\"Exact NN: {idx_exact[0][:5]}\")\n",
    "print(f\"Annoy:    {idx_annoy[0][:5]}\")\n",
    "print(f\"HNSW:     {idx_hnsw[0][:5]}\")\n",
    "print(f\"FAISS:    {idx_faiss[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319c1bd",
   "metadata": {},
   "source": [
    "## Buat dan tuliskan analisa anda terhadap code diatas.\n",
    "\n",
    "Code tersebut bertujuan membandingkan performa kecepatan dan hasil dari beberapa algoritma Nearest Neighbor Search pada dataset lirik lagu Spotify.\n",
    "\n",
    "#### Hasil dari code diatas:\n",
    "\n",
    "| Aspek                | Annoy                  | HNSW                   | FAISS              |\n",
    "| -------------------- | ---------------------- | ---------------------- | ------------------ |\n",
    "| **Kecepatan Query**  | Sedang                 | Tercepat               | Lambat             |\n",
    "| **Akurasi (Recall)** | Sangat tinggi (0.9945) | Sangat tinggi (0.9936) | Tertinggi (0.9982) |\n",
    "| **Waktu Build**      | Cepat                  | Sedang                 | Sangat cepat       |\n",
    "\n",
    "Interpretasi:\n",
    "- Exact NN terlalu lambat (brute force), tidak cocok untuk deployment real-time.\n",
    "- Annoy lebih ringan untuk build dan cocok untuk sistem yang butuh load cepat.\n",
    "- HNSW menawarkan keseimbangan terbaik: cepat, akurat, efisien.\n",
    "- FAISS IVF unggul dalam akurasi dan build speed, tetapi membutuhkan tuning (nlist, nprobe) agar query speed optimal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
